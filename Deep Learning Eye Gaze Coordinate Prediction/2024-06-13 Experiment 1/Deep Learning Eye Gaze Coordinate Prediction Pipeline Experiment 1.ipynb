{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Eye Gaze Pipeline Experiment 1\n",
    "\n",
    "Max Tran | Originally Published 6/13/2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What does this experiment do?**\n",
    "\n",
    "This experiment creates a realtime end-to-end pipeline that uses a raw webcam frame to predict the (x, y) coordinate a user is looking at on their screen.\n",
    "\n",
    "**How does this experiment work?**\n",
    "\n",
    "To create a coordinate prediction pipeline, three pre-trained OpenVINO models and two HAAR Cascade Classifiers are used in stages to progressively extract facial data from a raw webcam frame. The results of these networks are fed into a basic Dense DNN (Deep Neural Network) that estimates the on-screen coordinate a user is looking at. This final network is created and trained in this experiment. \n",
    "\n",
    "Data for the final Dense network is collected using an OpenCV window. When this window is clicked it will turn white and enter data collection mode. In this mode a user moves their mouse pointer around the screen and follows it with their eyes. Simulatenously, the coordiantes of the mouse are recorded and the pre-trained networks are used to extract and save facial data. This collected data can then be used to train the coordinate estimation model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here is a diagram of how the pipeline works:**\n",
    "\n",
    "![](https://mermaid.ink/img/pako:eNqtVF1v2jAU_StXfgJENgraCw-T-Gwr8SU6odFlaq-Sm2DNiSPHGaXAf5-T8JGm7cam-cW27rk-x773eMsc6RJrM0_ItbNCpWE0t0Mwo_PNZo_5Mh212oj_JFhwlyQMidxarRDsXs_hNkCfYKgwoEPo0Wbf81WsN4KgAx4Xoi0Vhj7ZYR7qVko8Q3To4w2hC33S5Gguwxdc04jCxe1kCtPxPYyNfFGIeibZco95liKNXFiNRuPTSVI1Xx35e1dlARn3TMYEg1jzAP9OwcpkW5HJtuiUbaGLcariqqjiwN8s84_I0zDY0G_u31vATaczh57AOOYeJ1VUgKgcjB106UGYs2hDD804ElzHH54C8YaEVlnCnPur_6ZBpYf9UUS_rOEanw09Vzn9P9XCN0e8VYbma_rBtkTfk1K5PERdbIMDQ1FAL4m1DKwvCnlIac-GcfpqFMGEEo3CTHot1Q-o9CeT6tkZ-yPzsGy0aWjdOYoohIKGmSKXvy5E5WsdltV3_DZ86bdi13fAsj5D92DBbLPLrAs9JaPIXEQbmxsz7YxBLkI1L0K1zrbLgZnZlriuw4xrZ1WHuRRiB4NDU-SgrBXMTbN5eZjvT6hBhhpeeHT_6LscdWpw6Hbl0znceifM6iwgFSB3zbe5TcE20ysKyGZts3TJw0Rom9nh3kAx0fJuEzqsrVVCdaZk4q9Y20MRm10Suaa2fY6-ea4jxNRZSzXO_-Xse97_AruOp28?type=png)\n",
    "\n",
    "<!-- (https://mermaid.live/edit#pako:eNqtVF1v2jAU_StXfgJENgraCw-T-Gwr8SU6odFlaq-Sm2DNiSPHGaXAf5-T8JGm7cam-cW27rk-x773eMsc6RJrM0_ItbNCpWE0t0Mwo_PNZo_5Mh212oj_JFhwlyQMidxarRDsXs_hNkCfYKgwoEPo0Wbf81WsN4KgAx4Xoi0Vhj7ZYR7qVko8Q3To4w2hC33S5Gguwxdc04jCxe1kCtPxPYyNfFGIeibZco95liKNXFiNRuPTSVI1Xx35e1dlARn3TMYEg1jzAP9OwcpkW5HJtuiUbaGLcariqqjiwN8s84_I0zDY0G_u31vATaczh57AOOYeJ1VUgKgcjB106UGYs2hDD804ElzHH54C8YaEVlnCnPur_6ZBpYf9UUS_rOEanw09Vzn9P9XCN0e8VYbma_rBtkTfk1K5PERdbIMDQ1FAL4m1DKwvCnlIac-GcfpqFMGEEo3CTHot1Q-o9CeT6tkZ-yPzsGy0aWjdOYoohIKGmSKXvy5E5WsdltV3_DZ86bdi13fAsj5D92DBbLPLrAs9JaPIXEQbmxsz7YxBLkI1L0K1zrbLgZnZlriuw4xrZ1WHuRRiB4NDU-SgrBXMTbN5eZjvT6hBhhpeeHT_6LscdWpw6Hbl0znceifM6iwgFSB3zbe5TcE20ysKyGZts3TJw0Rom9nh3kAx0fJuEzqsrVVCdaZk4q9Y20MRm10Suaa2fY6-ea4jxNRZSzXO_-Xse97_AruOp28) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A few steps are required to run this experiment:**\n",
    "\n",
    "1. I would recommend creating a virtual environment (venv) to isolate the environment for this experiment from your system installation of python. First, navigate in a terminal to the location you would like to create the virtual environment and then enter:\n",
    "\n",
    "        python -m venv .venv\n",
    "\n",
    "\n",
    "2. Follow Python's directions on how to [activate the venv for your platform](https://docs.python.org/3/library/venv.html#how-venvs-work). Then re-open your terminal and navigate back to your selected folder.\n",
    "\n",
    "2. Install Package Dependencies\n",
    "\n",
    "        pip install -r ./requirements.txt\n",
    "\n",
    "\n",
    "3. Download Pre-trained OpenVINO Models using the now installed Open Model Zoo (OMZ) downloader\n",
    "\n",
    "        omz_downloader --name face-detection-retail-0005\n",
    "\n",
    "\n",
    "        omz_downloader --name head-pose-estimation-adas-0001\n",
    "\n",
    "\n",
    "        omz_downloader --name gaze-estimation-adas-0002\n",
    "        \n",
    "\n",
    "4. Download a copy of the *haarcascade_lefteye_2splits.xml* and *haarcascade_righteye_2splits.xml* HAAR classifiers that can be be used to detect eyes. Copies can be found from the list of [OpenCV HAAR Classifiers](https://github.com/opencv/opencv/tree/4.x/data/haarcascades). Place these files in the same working directory as this notebook.\n",
    "\n",
    "5. To run the experiment, run all of the cells once from top to bottom. When you need to change modes, stop and start **the last cell only** to avoid erasing training data and trained models from your earlier runs. When in and *data collection and test* mode, press \"q\" on any window to exit and save your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Because this code is an experiment, it has not been fully formatted or cleaned up. It is also not yet production-ready.**\n",
    "\n",
    "**Additional work is required.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dependency Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Standard Library Imports\n",
    "from dataclasses import dataclass\n",
    "import os\n",
    "\n",
    "# Basic Data Handling and Display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Computer Vision\n",
    "import cv2 as cv\n",
    "\n",
    "# Machine Learning\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "# Machine Learning Inference Acceleration\n",
    "import openvino as ov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Variable\n",
    "# Stores a pre-trained model that predicts an (x, y)\n",
    "# coordinate on the screen based on a user's gaze.\n",
    "\n",
    "# In this experiment, the model is trained to accept the following parameters:\n",
    "# gx, gy, gz, yaw, pitch, roll\n",
    "\n",
    "# It then outputs a tensor with two normalized values (between 0 and 1) that can be\n",
    "# de-normalized and used as a coordinate on the screen.\n",
    "\n",
    "# In this current iteration of the experiment:\n",
    "# - The default screen size is assumed to be 1920x1080\n",
    "model = None\n",
    "\n",
    "\n",
    "# OpenCV window callbacks are used to record the user's cursor location for training \n",
    "# the coordinate prediction model.\n",
    "# Global variables are used by these callbacks to share this information.\n",
    "most_recent_x = None\n",
    "most_recent_y = None\n",
    "tracking = False\n",
    "\n",
    "screen_size = (1920, 1080)\n",
    "screen_size = (screen_size[1], screen_size[0])\n",
    "\n",
    "black_frame = np.zeros(screen_size)\n",
    "white_frame = np.ones(screen_size) * 255.0\n",
    "current_frame = black_frame\n",
    "\n",
    "\n",
    "# Global Variables are also used to store data collected for training.\n",
    "gx_collected_data = []\n",
    "gy_collected_data = []\n",
    "gz_collected_data = []\n",
    "\n",
    "yaw_collected_data = []\n",
    "pitch_collected_data = []\n",
    "roll_collected_data = []\n",
    "\n",
    "x_collected_data = []\n",
    "y_collected_data = []\n",
    "\n",
    "frames_collected_data = []\n",
    "\n",
    "# A path at which to save collected data\n",
    "DATA_SAVE_PATH = \"Data.csv\"\n",
    "\n",
    "# A Tunable Training Configuration\n",
    "@dataclass(frozen=True)\n",
    "class Training_Configuration():\n",
    "    batch_size : int = 256\n",
    "    epochs : int = 75\n",
    "\n",
    "    save_path = \"./Models\"\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam\n",
    "    \n",
    "    loss_function = tf.keras.losses.MeanSquaredError\n",
    "\n",
    "    proportion_of_data_for_validation = 0.2\n",
    "    shuffle = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Setup and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pretrained_xml_and_bin(model_name, precision):\n",
    "    \"\"\"Retrieve the .xml and .bin files (OpenVINO IR Format) from pretrained OpenVINO\n",
    "    models downloaded from the official model downloader.\n",
    "    \"\"\"\n",
    "    \n",
    "    base_path = f\"./intel\"\n",
    "\n",
    "    xml_file = os.path.join(base_path, model_name, precision, model_name + \".xml\")\n",
    "    bin_file = os.path.join(base_path, model_name, precision, model_name + \".bin\")\n",
    "    \n",
    "    return xml_file, bin_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bgr_to_rgb(image) -> np.ndarray:\n",
    "    \"\"\"Convert a BGR image to an RGB image.\"\"\"\n",
    "    \n",
    "    processed_image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
    "    return processed_image\n",
    "\n",
    "def show_image_inline(image):\n",
    "    \"\"\"Prepare and display a BGR image inside a Jupyter Notebook.\"\"\"\n",
    "\n",
    "    image = bgr_to_rgb(image)\n",
    "    image = image.astype(np.uint8)\n",
    "\n",
    "    plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_square_to_center(image, square_size, color=(255, 255, 255)):\n",
    "    \"\"\"Returns a copy of an image with an arbitrarily sized square drawn over its center.\"\"\"\n",
    "\n",
    "    y, x, channels = image.shape\n",
    "    half = square_size / 2\n",
    "\n",
    "    cord1 = int(x/2-half), int(y/2-half)\n",
    "    cord2 = int(x/2+half), int(y/2+half)\n",
    "\n",
    "    return cv.rectangle(image, cord1, cord2, color=color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_from_model(gx, gy, gz, yaw, pitch, roll, integer_response=False):\n",
    "    \"\"\"Inference on a pretrained model and convert its prediction to an (x, y) coordinate pair.\n",
    "\n",
    "    Currently, this function is hard-coded for the format of \n",
    "    \"\"\"\n",
    "\n",
    "    prediction = model.predict(np.array([\n",
    "        [gx, gy, gz, yaw, pitch, roll]\n",
    "    ]))[0]\n",
    "\n",
    "    x, y = prediction\n",
    "\n",
    "    x = x * 1920\n",
    "    y = y * 1080\n",
    "\n",
    "    if integer_response:\n",
    "        x = int(x)\n",
    "        y = int(y)\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_predictions_to_average = 5\n",
    "current_prediction_number = 0\n",
    "# This tensor stores accumulated predictions.\n",
    "predictions = np.zeros(shape=(20, 2))\n",
    "\n",
    "def average_predictions(gx, gy, gz, yaw, pitch, roll):\n",
    "    \"\"\"Performs a given number of model inference predictions and averages them before returning the result.\n",
    "\n",
    "    Each time this function is called it performs a single model inference and saves the result.\n",
    "    If a threshold number of predictions \"number_of_predictions_to_average\" is reached, all of the previously\n",
    "    stored predictions are averaged and the average is returned as an (x, y) coordinate pair.\n",
    "\n",
    "    Otherwise, the tuple (None, None) is returned.\n",
    "    \"\"\"\n",
    "\n",
    "    global number_of_predictions_to_average\n",
    "    global current_prediction_number\n",
    "    global predictions\n",
    "\n",
    "    x, y = predict_from_model(gx, gy, gz, yaw, pitch, roll)\n",
    "    predictions[current_prediction_number][0] = x\n",
    "    predictions[current_prediction_number][1] = y\n",
    "\n",
    "    # Accounting for zero-based indexing, check to see if the current prediction\n",
    "    # has filled the prediction buffer.\n",
    "    # If so, average the saved predictions and clear the buffer.\n",
    "    if current_prediction_number == number_of_predictions_to_average - 1:\n",
    "        x_mean = predictions[:, 0].mean()\n",
    "        y_mean = predictions[:, 1].mean()\n",
    "\n",
    "        current_prediction_number = 0\n",
    "        predictions = np.zeros(shape=(20, 2))\n",
    "\n",
    "        return int(x_mean), int(y_mean)\n",
    "\n",
    "    else:\n",
    "        current_prediction_number += 1\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes to Interact with Pre-trained Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A General and Extensible Class to Interact with OpenVINO Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class basic_model:\n",
    "\n",
    "    _MODEL_NAME = \"\"\n",
    "    _ANNOTATE_WITH_BBOXES = True\n",
    "\n",
    "    _compiled_model = None\n",
    "    _precision = \"FP32\"\n",
    "\n",
    "    _threshold = 0.5\n",
    "    _annotation_color = (255, 255, 255)\n",
    "    \n",
    "\n",
    "    def __init__(self, core, precision=\"FP32\", device=\"AUTO\", threshold=0.5, annotation_color=(255, 255, 255), return_annotated_frame=False):\n",
    "        \n",
    "        xml_file, _bin_file = get_pretrained_xml_and_bin(self._MODEL_NAME, precision)\n",
    "        self._compiled_model = core.compile_model(xml_file, device)\n",
    "        self._precision = precision\n",
    "\n",
    "        self._threshold = threshold\n",
    "        self._annotation_color = annotation_color\n",
    "        self._ANNOTATE_WITH_BBOXES = return_annotated_frame\n",
    "\n",
    "    def _preprocess_frame(self, frame):\n",
    "\n",
    "        # Make sure to take into account the precision here!\n",
    "        \n",
    "        return frame\n",
    "    \n",
    "    def inference(self, frame):\n",
    "        # Returns bboxes and an annotated preview frame\n",
    "\n",
    "        # OpenVino Inference Code\n",
    "        inference_request = self._compiled_model.create_infer_request()\n",
    "\n",
    "        processed_frame = self._preprocess_frame(frame)\n",
    "\n",
    "        input_tensor = ov.Tensor(processed_frame)\n",
    "        inference_request.set_input_tensor(input_tensor)\n",
    "\n",
    "        inference_request.start_async()\n",
    "        inference_request.wait()\n",
    "\n",
    "        output = inference_request.get_output_tensor()\n",
    "\n",
    "        # For models that return bboxes, this will be bboxes.\n",
    "        processed_output_data = self._retrieve_data_from_output(output)\n",
    "\n",
    "\n",
    "        if self._ANNOTATE_WITH_BBOXES:\n",
    "            return processed_output_data, self._annotate_frame_with_bboxes(frame, processed_output_data)\n",
    "        \n",
    "        else:\n",
    "            return processed_output_data\n",
    "    \n",
    "    def _retrieve_data_from_output(self, raw_output, scale_value=300):\n",
    "\n",
    "        # Returns bboxes if the model does so.\n",
    "        \n",
    "        # Use _threshold here\n",
    "        bboxes = []\n",
    "\n",
    "        return bboxes\n",
    "    \n",
    "    def _annotate_frame_with_bboxes(self, frame, bboxes):\n",
    "\n",
    "        preview_frame = frame\n",
    "        \n",
    "        for bbox in bboxes:\n",
    "            cv.rectangle(preview_frame, bbox[0], bbox[1], color=self._annotate_color)\n",
    "\n",
    "        return preview_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specific Model Classes Used in this Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Refactor the face_detection_retail Class Code\n",
    "# The code for this model class was originally written in a previous experiment\n",
    "# and is not as cleanly organized as the other model classes.\n",
    "\n",
    "class face_detection_retail:\n",
    "\n",
    "    _compiled_model = None\n",
    "\n",
    "    # A cropped-only frame that can still be used for previews.\n",
    "    cropped_frame = None\n",
    "\n",
    "    def __init__(self, core, precision, device=\"AUTO\"):\n",
    "\n",
    "        # Locate model files, compile the model, and save a reference inside the class.\n",
    "        xml_file, _bin_file = get_pretrained_xml_and_bin(\"face-detection-retail-0005\", precision)\n",
    "        self._compiled_model = core.compile_model(xml_file, device)\n",
    "\n",
    "    def _preprocess_frame(self, image):\n",
    "        \"\"\"Returns a frame ready for inference, and a preview-ready frame too.\"\"\"\n",
    "\n",
    "        # Original Shape is (480, 640, 3)\n",
    "        # Target input shape is (1,3,300,300)\n",
    "\n",
    "        # Crop a 300x300 Square from the center of the picture\n",
    "        y, x, channels = image.shape\n",
    "        image = image[int(y/2-150):int(y/2+150), int(x/2-150):int(x/2+150)]\n",
    "\n",
    "        # TODO: Figure out a cleaner way to allow this cropped frame to exit\n",
    "        self.cropped_frame = image\n",
    "\n",
    "        # During testing, OpenCV returns a UINT8 frame.\n",
    "        # Because OpenVINO is looking for FP32 frames, convert the frame here.\n",
    "        rearranged_image = image.astype(np.float32)\n",
    "\n",
    "        # Convert the dimension arrangement of the frame to the one the OpenVINO model is expecting.\n",
    "        # Move the channel dimension up and create a wrapper dimension at the front.\n",
    "        rearranged_image = np.moveaxis(rearranged_image, 2, 0)\n",
    "        rearranged_image = np.expand_dims(rearranged_image, 0)\n",
    "\n",
    "        return rearranged_image, image\n",
    "        \n",
    "\n",
    "    def inference(self, frame):\n",
    "        \"\"\"Returns bboxes and an annotated preview frame\"\"\"\n",
    "\n",
    "        # OpenVino Inference Code\n",
    "        inference_request = self._compiled_model.create_infer_request()\n",
    "\n",
    "        processed_frame, preview_frame = self._preprocess_frame(frame)\n",
    "\n",
    "        input_tensor = ov.Tensor(processed_frame)\n",
    "        inference_request.set_input_tensor(input_tensor)\n",
    "\n",
    "        inference_request.start_async()\n",
    "        inference_request.wait()\n",
    "\n",
    "        output = inference_request.get_output_tensor()\n",
    "        bboxes = self._retrieve_boxes_from_output(output)\n",
    "\n",
    "        return bboxes, self._annotate_frame(preview_frame, bboxes)\n",
    "        \n",
    "\n",
    "    def _retrieve_boxes_from_output(self, raw_output, threshold=0.5, scale_value=300):\n",
    "        \"\"\"Returns a list of tuples, each representing two coordinates for bounding boxes.\"\"\"\n",
    "        \n",
    "        output_buffer = raw_output.data\n",
    "        condensed_output = np.reshape(output_buffer, (200, 7)) \n",
    "\n",
    "        # Item 2 is the confidence level\n",
    "        thresholded_predictions = [pred for pred in condensed_output if pred[2] > threshold]\n",
    "\n",
    "        boxes = []\n",
    "\n",
    "        def prepare_cord(x, y, scale_value):\n",
    "            return (x * scale_value).astype(np.uint8), (y * scale_value).astype(np.uint8)\n",
    "\n",
    "        for prediction in thresholded_predictions:\n",
    "            cord1 = prepare_cord(prediction[3], prediction[4], scale_value)\n",
    "            cord2 = prepare_cord(prediction[5], prediction[6], scale_value)\n",
    "            coordinate_tuple = (cord1, cord2)\n",
    "\n",
    "            boxes.append(coordinate_tuple)\n",
    "        return boxes\n",
    "    \n",
    "    def _annotate_frame(self, frame, bboxes):\n",
    "        \"\"\"Annotate a frame with bounding box detections.\"\"\"\n",
    "\n",
    "        preview_frame = frame\n",
    "        \n",
    "        for bbox in bboxes:\n",
    "            cv.rectangle(preview_frame, bbox[0], bbox[1], color=(255, 255, 255))\n",
    "\n",
    "        return preview_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class head_pose_estimation(basic_model):\n",
    "\n",
    "    # This model requires an input tensor of shape (1, 3, 60, 60).\n",
    "\n",
    "    _MODEL_NAME = \"head-pose-estimation-adas-0001\"\n",
    "\n",
    "    def _preprocess_frame(self, frame):\n",
    "        \"\"\"Preprocess incoming frames for inference.\"\"\"\n",
    "\n",
    "        # Resize the frame\n",
    "        frame = cv.resize(frame, (60, 60))\n",
    "        \n",
    "        # Rearrange dimensions\n",
    "        rearranged_frame = np.moveaxis(frame, 2, 0)\n",
    "        rearranged_frame = np.expand_dims(rearranged_frame, 0)\n",
    "\n",
    "        # Cast to a floating point integer type\n",
    "        # TODO: Take into account user-selected model precision.\n",
    "        rearranged_frame = rearranged_frame.astype(np.float32)\n",
    "\n",
    "        return rearranged_frame\n",
    "    \n",
    "    def inference(self, frame):\n",
    "        \"\"\"Returns bboxes and an annotated preview frame.\"\"\"\n",
    "\n",
    "        # OpenVino Inference Code\n",
    "        inference_request = self._compiled_model.create_infer_request()\n",
    "\n",
    "        processed_frame = self._preprocess_frame(frame)\n",
    "\n",
    "        input_tensor = ov.Tensor(processed_frame)\n",
    "        inference_request.set_input_tensor(input_tensor)\n",
    "\n",
    "        inference_request.start_async()\n",
    "        inference_request.wait()\n",
    "\n",
    "        # Retrieve inference results without wrapping dimensions.\n",
    "        yaw = inference_request.get_output_tensor(0).data[0][0]\n",
    "        pitch = inference_request.get_output_tensor(1).data[0][0]\n",
    "        roll = inference_request.get_output_tensor(2).data[0][0]\n",
    "\n",
    "        return yaw, pitch, roll\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def locate_eyes(image, x1_y1_x2_y2_face_bounding_box=None):\n",
    "    \"\"\"Returns a tuple of tuples of tuples representing bounding box coordinates for the left and right eyes.\n",
    "     \n",
    "     This takes the format of (((x1, y1), (x2, y2)), ((x1, y1), (x2, y2)))\n",
    "     with the left eye bounding box coordinates listed followed by the right eye bounding box coordinates.\n",
    "     \"\"\"\n",
    "    \n",
    "    left_eye_cascade_filepath = r\"./haarcascade_lefteye_2splits.xml\"\n",
    "    right_eye_cascade_filepath = r\"./haarcascade_righteye_2splits.xml\"\n",
    "\n",
    "    left_eye_cascade = cv.CascadeClassifier()\n",
    "    right_eye_cascade = cv.CascadeClassifier()\n",
    "\n",
    "    # Load the classifiers from the OpenCV Sample Files\n",
    "    try:\n",
    "        left_eye_cascade.load(left_eye_cascade_filepath)\n",
    "        right_eye_cascade.load(right_eye_cascade_filepath)\n",
    "\n",
    "    except:\n",
    "        print(\"There was an error locating the HAAR classifiers.\")\n",
    "        return None\n",
    "    \n",
    "    # If necessary, crop the provided bounding box.\n",
    "    if x1_y1_x2_y2_face_bounding_box is not None:\n",
    "        (cord1, cord2) = x1_y1_x2_y2_face_bounding_box\n",
    "        x1, y1 = cord1\n",
    "        x1, y1 = cord2\n",
    "    \n",
    "        image = image[min(y1, y1):max(y1, y1), min(x1, x1):max(x1, x1)]\n",
    "    \n",
    "    # Detect eyes using the cascade classifiers.\n",
    "    left_eye_result = left_eye_cascade.detectMultiScale(image)\n",
    "    right_eye_result = right_eye_cascade.detectMultiScale(image)\n",
    "\n",
    "    # Extract more traditional coordinates to return\n",
    "    # ((x1, y1), (x2, y2))\n",
    "    left_eye_bbox_coordinates = None\n",
    "    right_eye_bbox_coordinates = None\n",
    "\n",
    "    if left_eye_result is not None:\n",
    "        x1, y1, w, h = left_eye_result[0]\n",
    "        left_eye_bbox_coordinates = ((x1, y1), (x1 + w, y1 + h))     \n",
    "\n",
    "    if right_eye_result is not None:\n",
    "        x1, y1, w, h = right_eye_result[0]\n",
    "        right_eye_bbox_coordinates = ((x1, y1), (x1 + w, y1 + h))\n",
    "\n",
    "    return (left_eye_bbox_coordinates, right_eye_bbox_coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gaze_estimation(basic_model):\n",
    "\n",
    "    # TODO: Describe required model inputs.\n",
    "\n",
    "    _MODEL_NAME = \"gaze-estimation-adas-0002\"\n",
    "\n",
    "    def _preprocess_frame(self, frame):\n",
    "\n",
    "        # Resize the frame\n",
    "        frame = cv.resize(frame, (60, 60))\n",
    "        \n",
    "        # Rearrange dimensions\n",
    "        rearranged_frame = np.moveaxis(frame, 2, 0)\n",
    "        rearranged_frame = np.expand_dims(rearranged_frame, 0)\n",
    "\n",
    "        # TODO: Take into account the user-selected model precision.\n",
    "        # Cast the numpy array to a floating point integer type\n",
    "        rearranged_frame = rearranged_frame.astype(np.float32)\n",
    "\n",
    "        return rearranged_frame\n",
    "    \n",
    "    def _preprocess_eye(self, eye_image):\n",
    "        eye_image = cv.resize(eye_image, (60, 60))\n",
    "\n",
    "        # Rearrange the array's dimensions into the format expected by the OpenVINO model.\n",
    "        rearranged_frame = np.moveaxis(eye_image, 2, 0)\n",
    "        rearranged_frame = np.expand_dims(rearranged_frame, 0)\n",
    "\n",
    "        rearranged_frame = rearranged_frame.astype(np.float32)\n",
    "\n",
    "        # Convert the numpy frame to an OpenVINO tensor.\n",
    "        tensor = ov.Tensor(rearranged_frame)\n",
    "\n",
    "        return tensor\n",
    "    \n",
    "    def inference(self, left_eye, right_eye, head_pose_data):\n",
    "        \"\"\"Returns bboxes and an annotated preview frame\"\"\"\n",
    "\n",
    "        # OpenVino Inference Code\n",
    "        inference_request = self._compiled_model.create_infer_request()\n",
    "\n",
    "        left_eye = self._preprocess_eye(left_eye)\n",
    "        right_eye = self._preprocess_eye(right_eye)\n",
    "\n",
    "        head_pose_data = np.array(head_pose_data)\n",
    "        head_pose_data = np.expand_dims(head_pose_data, 0)\n",
    "        head_pose_data = ov.Tensor(head_pose_data)\n",
    "\n",
    "        inference_request.set_input_tensor(0, left_eye)\n",
    "        inference_request.set_input_tensor(1, right_eye)\n",
    "        inference_request.set_input_tensor(2, head_pose_data)\n",
    "\n",
    "        inference_request.start_async()\n",
    "        inference_request.wait()\n",
    "\n",
    "        # Obtain the inference result.\n",
    "        raw_output = inference_request.get_output_tensor()\n",
    "\n",
    "        # Example output of data:\n",
    "        # array([[ 0.02891541, -0.28149414, -0.9379883 ]], dtype=float32)\n",
    "        return raw_output.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Coordinate Prediction Pipeline\n",
    "\n",
    "This pipeline uses specified pre-trained models and the user-trained (x, y) coordinate model to create an end-to-end gaze coordinate prediction pipeline.\n",
    "\n",
    "This pipeline is under heavy development; the version below was written as an experiment and is not yet production-ready.\n",
    "\n",
    "As pieces have been added and removed over the course of the experiment, comments and code pieces have not yet been cleaned up or formatted for helpful viewing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class coordinate_prediction_pipeline:\n",
    "\n",
    "    # TODO: Clean up try and except block handling.\n",
    "\n",
    "    core = None\n",
    "\n",
    "    face_detection_model = None\n",
    "    head_pose_estimation_model = None\n",
    "    gaze_estimation_model = None\n",
    "\n",
    "    def __init__(self, core):\n",
    "        self.core = core\n",
    "\n",
    "        self.face_detection_model = face_detection_retail(core, \"FP32\", device=\"AUTO\")\n",
    "        self.head_pose_estimation_model = head_pose_estimation(core, \"FP32\", device=\"AUTO\")\n",
    "        self. gaze_estimation_model = gaze_estimation(core, \"FP32\", device=\"GPU\")\n",
    "\n",
    "    def inference(self, frame):\n",
    "\n",
    "        # \n",
    "        # Stage 1: Locate Faces\n",
    "        # \n",
    "        bboxes, annotated_frame = self.face_detection_model.inference(frame)\n",
    "\n",
    "        # Continue early if no faces were detected\n",
    "        if len(bboxes) <= 0:\n",
    "            return (None, None, None, None)\n",
    "\n",
    "        # Display an annotated frame with the first located face\n",
    "        # TODO: Optimize the face detection model class\n",
    "        cv.imshow(\"Face Detections\", annotated_frame)\n",
    "\n",
    "        unannotated_cropped_frame = self.face_detection_model.cropped_frame\n",
    "\n",
    "        # Crop the unannotated frame to the first face.\n",
    "        # TODO: Handle multiple faces\n",
    "        # Images are stored y, x.\n",
    "\n",
    "        # Decompose the classes's bbox format.\n",
    "        (cord1, cord2) = bboxes[0]\n",
    "        x1, y1 = cord1\n",
    "        x2, y2 = cord2\n",
    "\n",
    "        frame_cropped_to_face = unannotated_cropped_frame[min(y1, y2):max(y1, y2), min(x1, x2):max(x1, x2)]\n",
    "\n",
    "        try:\n",
    "            yaw, pitch, roll = self.head_pose_estimation_model.inference(frame_cropped_to_face)\n",
    "        except:\n",
    "            return (None, None, None, None)\n",
    "\n",
    "        try:\n",
    "            left_bbox_coordinates, right_bbox_coordinates = locate_eyes(frame_cropped_to_face)\n",
    "            if (left_bbox_coordinates is None) and (right_bbox_coordinates is None):\n",
    "                return (None, None, None, None)\n",
    "            \n",
    "        except:\n",
    "            return (None, None, None, None)\n",
    "\n",
    "        # Try to display the image cropped to the active user's face.\n",
    "        # TODO: Determine why this sometimes fails.\n",
    "        try:\n",
    "            annotation = cv.rectangle(frame_cropped_to_face, left_bbox_coordinates[0], left_bbox_coordinates[1], color=(255, 255, 255))\n",
    "            annotation = cv.rectangle(frame_cropped_to_face, right_bbox_coordinates[0], right_bbox_coordinates[1], color=(255, 255, 255))\n",
    "            cv.imshow(\"Active Face\", annotation)\n",
    "        except:\n",
    "            return (None, None, None, None)\n",
    "\n",
    "        (x1, y1), (x2, y2) = left_bbox_coordinates\n",
    "        eye1_image = frame_cropped_to_face[min(y1, y2):max(y1, y2), min(x1, x2):max(x1, x2)]\n",
    "        \n",
    "        (x1, y1), (x2, y2) = right_bbox_coordinates\n",
    "        eye2_image = frame_cropped_to_face[min(y1, y2):max(y1, y2), min(x1, x2):max(x1, x2)]\n",
    "\n",
    "        gaze_direction = self.gaze_estimation_model.inference(eye1_image, eye2_image, (yaw, pitch, roll))\n",
    "\n",
    "        return (gaze_direction, yaw, pitch, roll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection and Testing/Training Modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_event_handler(event, x, y, flags, param):\n",
    "    \"\"\"An OpenCV Window Callback to Collect Mouse Data\"\"\"\n",
    "\n",
    "    global most_recent_x\n",
    "    global most_recent_y\n",
    "    global tracking\n",
    "\n",
    "    if event == cv.EVENT_LBUTTONDOWN:\n",
    "\n",
    "        # Invert the tracking variable\n",
    "        tracking = not tracking\n",
    "    \n",
    "    if tracking:\n",
    "        most_recent_x = x\n",
    "        most_recent_y = y\n",
    "\n",
    "def collect_data_and_test_current_model():\n",
    "\n",
    "    global model\n",
    "    global tracking\n",
    "\n",
    "    pipeline = coordinate_prediction_pipeline(core)\n",
    "\n",
    "    WINDOW_NAME = \"Frame\"\n",
    "\n",
    "    tracking = False\n",
    "\n",
    "    current_x_mean = 0\n",
    "    current_y_mean = 0\n",
    "\n",
    "    while True:\n",
    "        window = cv.namedWindow(WINDOW_NAME, cv.WND_PROP_FULLSCREEN)\n",
    "        cv.setWindowProperty(WINDOW_NAME, cv.WND_PROP_FULLSCREEN, cv.WND_PROP_FULLSCREEN)\n",
    "\n",
    "        cv.setMouseCallback(WINDOW_NAME, window_event_handler)\n",
    "\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            print(\"There was an error accessing your camera. Additional investigation is required.\")\n",
    "            break\n",
    "\n",
    "        data_present = False\n",
    "        if (not frame.any()) or frame is None:\n",
    "            cv.waitKey(1)\n",
    "\n",
    "        else:\n",
    "            # At this point, a frame should be ready.\n",
    "            frame_with_rectangle = add_square_to_center(frame, 300)\n",
    "            cv.imshow(\"Live Preview\", frame_with_rectangle)\n",
    "\n",
    "\n",
    "            gaze_direction, yaw, pitch, roll = pipeline.inference(frame)\n",
    "            if gaze_direction is not None:\n",
    "                gx = gaze_direction[0]\n",
    "                gy = gaze_direction[1]\n",
    "                gz = gaze_direction[2]\n",
    "\n",
    "            if((gaze_direction is None) or (yaw is None) or (pitch is None) or (roll is None)):\n",
    "                cv.waitKey(1)\n",
    "            else:\n",
    "                data_present = True\n",
    "\n",
    "        # If all data is present and a model is available, try to inference now.\n",
    "        if data_present and (model is not None):\n",
    "            # x_mean, y_mean = average_predictions(gx, gy, gz, yaw, pitch, roll)\n",
    "            x_mean, y_mean = predict_from_model(gx, gy, gz, yaw, pitch, roll, integer_response=True)\n",
    "\n",
    "            if (x_mean is not None) and (y_mean is not None):\n",
    "                current_x_mean = x_mean\n",
    "                current_y_mean = y_mean\n",
    "\n",
    "        # All data is present!\n",
    "        if tracking:\n",
    "            \n",
    "            white_frame_copy = white_frame.copy()\n",
    "            \n",
    "            starting_offset = (30, 80)\n",
    "            other_options = (cv.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 4, cv.LINE_AA)\n",
    "            cv.putText(white_frame_copy, f\"Current Mouse Location: ({most_recent_x}, {most_recent_y})\", starting_offset, *other_options)\n",
    "\n",
    "            starting_offset = (30, 200)\n",
    "            other_options = (cv.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 4, cv.LINE_AA)\n",
    "            cv.putText(white_frame_copy, f\"Number of data points: {len(x_collected_data)}\", starting_offset, *other_options)\n",
    "\n",
    "            if data_present and model is not None:\n",
    "                cv.circle(white_frame_copy, (current_x_mean, current_y_mean), radius=15, color=(0, 0, 0), thickness=-1)\n",
    "\n",
    "            cv.imshow(WINDOW_NAME, white_frame_copy)\n",
    "\n",
    "            # If all data is present, record it all.\n",
    "            if data_present:\n",
    "\n",
    "                gx_collected_data.append(gx)\n",
    "                gy_collected_data.append(gy)\n",
    "                gz_collected_data.append(gz)\n",
    "                \n",
    "                yaw_collected_data.append(yaw)\n",
    "                pitch_collected_data.append(pitch)\n",
    "                roll_collected_data.append(roll)\n",
    "\n",
    "                x_collected_data.append(most_recent_x)\n",
    "                y_collected_data.append(most_recent_y)\n",
    "\n",
    "        else:\n",
    "            black_frame_copy = black_frame.copy()\n",
    "\n",
    "            starting_offset = (30, 200)\n",
    "            other_options = (cv.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 4, cv.LINE_AA)\n",
    "            cv.putText(black_frame_copy, f\"Number of data points: {len(x_collected_data)}\", starting_offset, *other_options)\n",
    "            if data_present and model is not None:\n",
    "                cv.circle(black_frame_copy, (current_x_mean, current_y_mean), radius=15, color=(255, 255, 255), thickness=-1)\n",
    "            cv.imshow(WINDOW_NAME, black_frame_copy)\n",
    "            \n",
    "\n",
    "        if cv.waitKey(1) == ord(\"q\"):\n",
    "            cv.destroyAllWindows()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data():\n",
    "    new_df = pd.DataFrame(\n",
    "        {\n",
    "            \"gx_data\" : gx_collected_data,\n",
    "            \"gy_data\" : gy_collected_data,\n",
    "            \"gz_data\" : gz_collected_data,\n",
    "\n",
    "            \"yaw_data\" : yaw_collected_data,\n",
    "            \"pitch_data\" : pitch_collected_data,\n",
    "            \"roll_data\" : roll_collected_data,\n",
    "\n",
    "            \"x_data\" : x_collected_data,\n",
    "            \"y_data\" : y_collected_data\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # If no data has been saved before, create and save a new data frame.\n",
    "    if not os.path.exists(DATA_SAVE_PATH):\n",
    "\n",
    "        new_df.to_csv(DATA_SAVE_PATH)\n",
    "        \n",
    "        df = new_df\n",
    "\n",
    "        print(f\"Your data has been saved to a new data frame at {DATA_SAVE_PATH}!\")\n",
    "\n",
    "    # Otherwise, load the existing frame, append data to it, and save it.\n",
    "    else:\n",
    "\n",
    "        df = pd.read_csv(DATA_SAVE_PATH, index_col=0)\n",
    "        df = pd.concat([df, new_df], ignore_index=True)\n",
    "\n",
    "        df.to_csv(DATA_SAVE_PATH)\n",
    "\n",
    "        print(f\"Your data has been appended to an existing data frame at {DATA_SAVE_PATH}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "\n",
    "    global model\n",
    "    \n",
    "    df = pd.read_csv(DATA_SAVE_PATH, index_col=0)\n",
    "\n",
    "    # Convert each column of the training data to NumPy arrays.\n",
    "    gx_data = df[\"gx_data\"].to_numpy()\n",
    "    gy_data = df[\"gy_data\"].to_numpy()\n",
    "    gz_data = df[\"gz_data\"].to_numpy()\n",
    "    yaw_data = df[\"yaw_data\"].to_numpy()\n",
    "    pitch_data = df[\"pitch_data\"].to_numpy()\n",
    "    roll_data = df[\"roll_data\"].to_numpy()\n",
    "\n",
    "    x_data = df[\"x_data\"].to_numpy()\n",
    "    y_data = df[\"y_data\"].to_numpy()\n",
    "\n",
    "    # Create x and y training datasets.\n",
    "    input_data = np.array([np.array([gx, gy, gz, yaw, pitch, roll]) for gx, gy, gz, yaw, pitch, roll in zip(gx_data, gy_data, gz_data, yaw_data, pitch_data, roll_data)])\n",
    "    output_data = np.array([np.array([x/1920., y/1080.]) for x, y in zip(x_data, y_data)])\n",
    "\n",
    "    assert len(input_data) == len(output_data), f\"We're not sure why, but the length of your x_train data ({len(input_data)}) does not match the length of your y_train data ({len(input_data)}). Make sure these are the same length or delete your data (stored in the Data.csv file) and try collecting it again.\"\n",
    "\n",
    "    input = keras.Input(shape=(6,), name=\"Inputs\")\n",
    "\n",
    "    # Define a basic Keras Model to Predict Screen Coordinates.\n",
    "\n",
    "    # Normalize each of the model's inputs.\n",
    "    norm_layer = keras.layers.Normalization(name=\"Normalization-Layer\")\n",
    "    norm_layer.adapt(input_data)\n",
    "    x = norm_layer(input)\n",
    "\n",
    "    x = keras.layers.Flatten()(x)\n",
    "\n",
    "    x = keras.layers.Dense(512, activation=\"relu\")(x)\n",
    "    x = keras.layers.Dense(256, activation=\"relu\")(x)\n",
    "    x = keras.layers.Dense(128, activation=\"relu\")(x)\n",
    "    x = keras.layers.Dense(64, activation=\"relu\")(x)\n",
    "\n",
    "    output = keras.layers.Dense(2, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = keras.Model(inputs=input, outputs=output)\n",
    "\n",
    "    # Print out a model summary.\n",
    "    model.summary()\n",
    "\n",
    "    # Compile the model for training.\n",
    "    training_config = Training_Configuration()\n",
    "\n",
    "    model.compile(training_config.optimizer(),\n",
    "                        loss=training_config.loss_function(),\n",
    "                        metrics=[\"accuracy\"])\n",
    "    \n",
    "    # Train the Model\n",
    "    model.fit(x=input_data,\n",
    "          y=output_data,\n",
    "          epochs=training_config.epochs,\n",
    "          batch_size=training_config.batch_size,\n",
    "          validation_split=training_config.proportion_of_data_for_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core = ov.Core()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open an OpenCV VideoCapture Instance\n",
    "try:\n",
    "    cap = cv.VideoCapture(0)\n",
    "    # Prime the camera by reading a few initial frames.\n",
    "    for _ in range(0, 60):\n",
    "        _, _ = cap.read()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Running Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "\n",
    "    # A separate user input loop\n",
    "    user_response = None\n",
    "    while(True):\n",
    "        user_response = int(input(\"What would you like to do?\\n\\t1. Collect Training Data and Test Your Currently Trained Model\\n\\t2. Train a Model from Collected Training Data\\n\\t3. Exit\\n\"))\n",
    "        \n",
    "        if not user_response in range(1, 4):\n",
    "            print(\"Please try again and select a valid option.\\n\")\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    if user_response == 1:\n",
    "        collect_data_and_test_current_model()\n",
    "        save_data()\n",
    "\n",
    "    elif user_response == 2:\n",
    "        train_model()\n",
    "    \n",
    "    else:\n",
    "        save_data()\n",
    "        cap.release()\n",
    "        \n",
    "        print(\"The camera has been turned off and any remaining data has been saved You're good to go!\")\n",
    "        \n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
